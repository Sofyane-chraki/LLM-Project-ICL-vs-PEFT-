# Project Contribution

![image](https://github.com/Sofyane-chraki/LLM-Project-ICL-vs-PEFT-/assets/91961463/c32f27fd-0bd7-43a3-9b36-220b9c3b5dc6)


# LLM-Project-ICL-vs-PEFT-
Here's the repository for the LLM project. Its purpose is to train a Large Language Model (LLM) for question-answering about a specific topic. Additionally, we aim to compare two methods of model training and understand the advantages and drawbacks of each.

The project is structured as follows:

from_pdf_to_csv.py: This file is dedicated to modifying and initially creating our dataset. Originally, its purpose was to extract data in the format of questions and answers and create a CSV file from it.
As we progressed with the project, we modified our dataset to better suit our objectives. This involved adapting the format of the dataset to fit our two methodologies and meet the requirements of the model.

Then we have two jupyter notebook file, each one of them is dedicated for one method.

The "PEFT.ipynb" file encapsulates the culmination of our collaborative efforts. It represents a comprehensive integration of various techniques and methodologies aimed at fine-tuning a Mistral-7B model using 4-bit quantization and QLora training. This notebook serves as a detailed guide, providing step-by-step instructions on preprocessing data, configuring the model architecture, defining hyperparameters, and training the model. 
Additionally, it includes sections on saving the trained model, evaluating its performance, and deploying it for inference tasks. 
By making this notebook available in the GitHub repository, stakeholders can access a structured and executable document that not only demonstrates the implementation of advanced machine learning techniques but also fosters further collaboration and learning within the project team and the broader community.

Finaly we have the dataset in the txt format.
