{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8138303,"sourceType":"datasetVersion","datasetId":4811266},{"sourceId":8138980,"sourceType":"datasetVersion","datasetId":4811773},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers pypdf pipeline torch","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:25:00.812638Z","iopub.execute_input":"2024-04-17T15:25:00.813512Z","iopub.status.idle":"2024-04-17T15:27:45.758351Z","shell.execute_reply.started":"2024-04-17T15:25:00.813470Z","shell.execute_reply":"2024-04-17T15:27:45.757225Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:27:45.760325Z","iopub.execute_input":"2024-04-17T15:27:45.760638Z","iopub.status.idle":"2024-04-17T15:27:45.765589Z","shell.execute_reply.started":"2024-04-17T15:27:45.760590Z","shell.execute_reply":"2024-04-17T15:27:45.764604Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:27:45.766774Z","iopub.execute_input":"2024-04-17T15:27:45.767117Z","iopub.status.idle":"2024-04-17T15:28:03.846022Z","shell.execute_reply.started":"2024-04-17T15:27:45.767088Z","shell.execute_reply":"2024-04-17T15:28:03.845251Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-17 15:27:51.337252: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-17 15:27:51.337415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-17 15:27:51.555713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,  # Pass torch.bfloat16 here\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:28:03.848153Z","iopub.execute_input":"2024-04-17T15:28:03.848708Z","iopub.status.idle":"2024-04-17T15:31:03.486767Z","shell.execute_reply.started":"2024-04-17T15:28:03.848680Z","shell.execute_reply":"2024-04-17T15:31:03.485891Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef066368ad1c476a8d953f3ffcabbfc4"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\npipeline = pipeline(\n    task='text-generation',\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,  # needed by langchain\n    # model params\n    max_new_tokens=256,\n    #temperature=0.1,  # creativity of responses: 0.0 none ->  1.0 max\n    repetition_penalty=1.1  # to avoid repeating output\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:03.488049Z","iopub.execute_input":"2024-04-17T15:31:03.488409Z","iopub.status.idle":"2024-04-17T15:31:03.493827Z","shell.execute_reply.started":"2024-04-17T15:31:03.488363Z","shell.execute_reply":"2024-04-17T15:31:03.492879Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(type(model))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:03.494916Z","iopub.execute_input":"2024-04-17T15:31:03.495177Z","iopub.status.idle":"2024-04-17T15:31:03.508230Z","shell.execute_reply.started":"2024-04-17T15:31:03.495155Z","shell.execute_reply":"2024-04-17T15:31:03.507349Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(base_model); config","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:03.509345Z","iopub.execute_input":"2024-04-17T15:31:03.509646Z","iopub.status.idle":"2024-04-17T15:31:03.525570Z","shell.execute_reply.started":"2024-04-17T15:31:03.509622Z","shell.execute_reply":"2024-04-17T15:31:03.524737Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"MistralConfig {\n  \"_name_or_path\": \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\nllm = HuggingFacePipeline(pipeline=pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:03.526472Z","iopub.execute_input":"2024-04-17T15:31:03.526809Z","iopub.status.idle":"2024-04-17T15:31:03.891424Z","shell.execute_reply.started":"2024-04-17T15:31:03.526786Z","shell.execute_reply":"2024-04-17T15:31:03.890662Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader('/kaggle/input/filess/Question_Answering_Dataset.pdf')\ndocuments = loader.load_and_split()","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:03.892352Z","iopub.execute_input":"2024-04-17T15:31:03.892617Z","iopub.status.idle":"2024-04-17T15:31:05.701708Z","shell.execute_reply.started":"2024-04-17T15:31:03.892595Z","shell.execute_reply":"2024-04-17T15:31:05.700881Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\ntexts = text_splitter.split_documents(documents)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:05.702785Z","iopub.execute_input":"2024-04-17T15:31:05.703071Z","iopub.status.idle":"2024-04-17T15:31:05.711544Z","shell.execute_reply.started":"2024-04-17T15:31:05.703048Z","shell.execute_reply":"2024-04-17T15:31:05.710693Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# loading the FAISS vector database\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\n# This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space\n# and can be used for tasks like clustering or semantic search.\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': False}\nembeddings = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n# storing embeddings in the vector store\nvectorstore = FAISS.from_documents(texts, embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:05.714063Z","iopub.execute_input":"2024-04-17T15:31:05.714336Z","iopub.status.idle":"2024-04-17T15:31:23.321165Z","shell.execute_reply.started":"2024-04-17T15:31:05.714315Z","shell.execute_reply":"2024-04-17T15:31:23.320422Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6b32676d5e84d5c9472fb447a2d73a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a8cb3ef967a4786a1bc9c3746fdb33b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f11269e61b48ffa37ac6fa3ab166f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33075bf80cf149068c2af9878619aa3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caaf714fb7fc44ae8d68b451999ee6b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52004791ed544eb3951162137503cab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3690a840fbe4f4da3cb35e0a30037fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187455855536431bb0aaf9348e9841ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd43b1c665ff4fa3a80255af39d6b3b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea57590672b548ba8aabd759224a90a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14291a116c7c4296a3a6a49ddbe20010"}},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\nfrom langchain import PromptTemplate\ntt = PromptTemplate.from_template(\"\"\"Act as professor and answer very clearly to question asked, and be very clear and detailed<>\n{content}]\"\"\")\n\nchain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:23.322181Z","iopub.execute_input":"2024-04-17T15:31:23.322440Z","iopub.status.idle":"2024-04-17T15:31:23.562850Z","shell.execute_reply.started":"2024-04-17T15:31:23.322418Z","shell.execute_reply":"2024-04-17T15:31:23.562063Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"query = \"What is the assessment strategy for the course Information Systems and Technologies?\"\nchat_history = []\nresult = chain({\"question\": tt.format(content=query), \"chat_history\": chat_history})\ndef process_answer(answer):\n    lines = answer.split(\"\\n\")\n    for line in lines:\n        if \"Question:\" in line:\n            break\n        print(line.strip())\nprocess_answer(result['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:31:23.563856Z","iopub.execute_input":"2024-04-17T15:31:23.564138Z","iopub.status.idle":"2024-04-17T15:31:44.677654Z","shell.execute_reply.started":"2024-04-17T15:31:23.564113Z","shell.execute_reply":"2024-04-17T15:31:44.676524Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"name":"stdout","text":"\n\nThe assessment strategy includes a 2 -hour written examination, a 30 -minute group presentation on emerging technologies, and a mini-project completed b y groups proposing solutions for real -case problems.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Combien de crédits sont associés au cours Systèmes et technologies de l'information?\" \nchat_history = []\nresult = chain({\"question\": tt.format(content=query), \"chat_history\": chat_history})\ndef process_answer(answer):\n    lines = answer.split(\"\\n\")\n    for line in lines:\n        if \"Question:\" in line:\n            break\n        print(line.strip())\nprocess_answer(result['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:42:23.783443Z","iopub.execute_input":"2024-04-17T15:42:23.783834Z","iopub.status.idle":"2024-04-17T15:42:41.345336Z","shell.execute_reply.started":"2024-04-17T15:42:23.783807Z","shell.execute_reply":"2024-04-17T15:42:41.344401Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\nThe course offers 4 ECTS credits.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Qui est le vainqueur de la coupe du monde de football en 2018?\"\nchat_history = []\nresult = chain({\"question\": tt.format(content=query), \"chat_history\": chat_history})\nprint(result['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-04-17T15:43:05.594824Z","iopub.execute_input":"2024-04-17T15:43:05.595200Z","iopub.status.idle":"2024-04-17T15:43:25.312659Z","shell.execute_reply.started":"2024-04-17T15:43:05.595171Z","shell.execute_reply":"2024-04-17T15:43:25.311661Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\n\nQuestion: Act as professor and answer very clearly to question asked, and be very clear and detailed<>\nQui est le vainqueur de la coupe du monde de football en 2018?]\nHelpful Answer:\n\nQuestion: Act as professor and answer very clearly to question asked, and be very clear and detailed<>\nQui est le vainqueur de la coupe du monde de football en 2018?]\nHelpful Answer:\n\nQuestion: Act as professor and answer very clearly to question asked, and be very clear and detailed<>\nQui est le vainqueur de la coupe du monde de football en 2018?]\nHelpful Answer:\n\nQuestion: Act as professor and answer very clearly to question asked, and be very clear and detailed<>\nQui est le vainqueur de la coupe du monde de football en 2018?]\nHelpful Answer:\n\nQuestion: Act as professor and answer very clearly to question asked, and be very clear and detailed<>\nQui est le vainqueur de la coupe du monde de football en 2018?]\nHelpful Answer:\n\n","output_type":"stream"}]}]}